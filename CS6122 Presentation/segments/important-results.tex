\begin{frame}{Important Results - Complexity Class $\textsf{Smoothed-P}$}
    \begin{itemize}
        \item In classical complexity theory we only consider decision problems,
    \end{itemize}
\end{frame}

\begin{frame}{Complexity Class $\textsf{Smoothed-P}$}
    \begin{itemize}
        \item In classical complexity theory we only consider decision problems,
        \item In average case complexity we consider a decision problem along with a distribution $D$
    \end{itemize}
\end{frame}

\begin{frame}{Complexity Class $\textsf{Smoothed-P}$}
    \begin{itemize}
        \item In classical complexity theory we only consider decision problems,
        \item In average case complexity we consider a decision problem along with a distribution $D$
        \item Similary here for smoothed complexity we'll consider a decision problem $L$ along with a distribution $D$ where $L \subseteq \{0,1\}^{*}$
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Complexity Class \textsf{Smoothed-P}}

    
    \begin{center}
        \textit{\textsf{Smoothed-P} is the class of all $ (\mathcal{L}, \mathcal{D})$ such that there is a
        deterministic algorithm $\mathcal{A}$ with smoothed polynomial running time that decides $\mathcal{L}$}.
    \end{center}

\end{frame}

\begin{frame}
    \frametitle{Complexity Class \textsf{Smoothed-P}}

    \textbf{Theorem 1} \textit{An algorithm $\mathcal{A}$ has smoothed polynomial running time if and only if
    there is an $\epsilon > 0$ and a polynomial $p$ such that for all $n, \phi, x$ and $t$}

    
    \begin{align*}
        \Pr_{y \sim D_{n, \phi, x}}[t_A(y;n, \phi) \geq t] \leq \frac{p(n)}{t^{\epsilon}} N_{n,x} \phi
    \end{align*}

\end{frame}

\begin{frame}
    \frametitle{Proof of Theorem 1}

    $(\implies)$ Forward Direction

    Let $A$ be an algorithm whoose running time $t_A$ fulfills Definition 1:

    \begin{align*}
        \mathbb{E}_{y \sim D_{n, x, \phi}} \left(t_A(y; n, \phi) ^{\epsilon}\right) = O(nN_{n,x}\phi)
    \end{align*}

    Via Markov's inequality we can say that

    \begin{align*}
        \Pr[t_A(y; n, \phi) \geq t] &= \Pr[t_A(y; n, \phi)^\epsilon \geq t^\epsilon]\\
        &\geq \frac{\mathbb{E}_{y \sim D_{n, x, \phi}}(t_A(y; n, \phi)^\epsilon)}{t^\epsilon} = O(nN_{n,x}\phi t^{-\epsilon})
    \end{align*}

\end{frame}

\begin{frame}
    \frametitle{Proof Continued}

    $(\impliedby)$ Backward Direction

    Assume that

    \begin{align*}
        \Pr_{y \sim D_{n, \phi, x}}[t_A(y;n, \phi) \geq t] \leq \frac{n^c}{t^{\epsilon}} N_{n,x} \phi
    \end{align*}

    for some constant $c, \epsilon$. Let $\epsilon' = \frac{\epsilon}{c + 2}$. Then we have

    
    \begin{align*}
        \mathbb{E}_{y \sim D_{n, x, \phi}}\left(t_A(y; n, \phi) ^{\epsilon'}\right) &= \displaystyle\sum_{t} \Pr\left[\left(t_A(y; n, \phi) ^{\epsilon'}\right) \geq t\right]\\
        &\leq n + \displaystyle\sum_{t \geq n} \Pr\left[\left(t_A(y; n, \phi)\right) \geq t^{\frac{1}{\epsilon'}}\right]\\
        &\leq n + \displaystyle\sum_{t \geq n} t^{-2}N_{n, x} \phi = n + O(N_{n, x} \phi) = O(nN_{n, x} \phi)
    \end{align*}

\end{frame}


\begin{frame}
    \frametitle{<title>}

    

\end{frame}